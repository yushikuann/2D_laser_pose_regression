{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import skimage.data\n",
    "from skimage import io\n",
    "import skimage.transform\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"input\"):\n",
    "    input_image = tf.placeholder(\"float\", [None, 448,448,3],name='input_image')\n",
    "#     input_image = tf.pad(image,np.array([[0, 0], [3, 3], [3, 3], [0, 0]]))\n",
    "    y_ = tf.placeholder(\"float\", [None, 3],name='y_') \n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    #卷积函数实现卷积层的前向传播\n",
    "    #其中第一个参数ｘ为当前节点的矩阵，为一个４维矩阵\n",
    "    #第二个参数为卷积核的值\n",
    "    #第三个参数为不同维度的步长第一个和最后一个参数必须为１\n",
    "    #第四个参数为填充，SAME为０添加，VALLD为不添加\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    #返回值仍然是一个４维的tensor\n",
    "    #第一维为batch数量\n",
    "    #第二维和第三维表示卷积层的维度，由输入层和卷集核以及移动步长共同决定\n",
    "    #第四维的参数为自己定义的卷积层的深度\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "#第一层卷积层\n",
    "#参数的前两维为卷积核的维度，第三个参数为当前层的深度，第四个为输出到层的深度\n",
    "W_conv1 = weight_variable([7, 7, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(input_image, W_conv1,strides=[1,2,2,1],padding=\"SAME\") + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)   #输出变成112*112*64\n",
    "\n",
    "#第二层卷积层\n",
    "W_conv2 = weight_variable([3, 3, 64, 192])\n",
    "b_conv2 = bias_variable([192])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)  #输出变为56*56*192\n",
    "\n",
    "\n",
    "#第三层卷积层\n",
    "W_conv3 = weight_variable([3, 3, 192, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)   #输出为28*28*128\n",
    "\n",
    "\n",
    "#第四层卷积层\n",
    "W_conv4 = weight_variable([3, 3, 128, 64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_pool3, W_conv4) + b_conv4)\n",
    "h_pool4 = max_pool_2x2(h_conv4)   #输出变为14*14*64\n",
    "\n",
    "\n",
    "#密集连接层1\n",
    "W_fc1 = weight_variable([14*14*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool4, [-1, 14*14*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)   #输出为96*1024\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "#relu函数作用是求出max(h_fc1,0)\n",
    "h_fc1_drop1 = tf.nn.dropout(h_fc1, keep_prob)   #输出为96*1024\n",
    "\n",
    "\n",
    "#密集连接层2\n",
    "W_fc2 = weight_variable([1024, 512])\n",
    "b_fc2 = bias_variable([512])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop1, W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "#relu函数作用是求出max(h_fc1,0)\n",
    "h_fc1_drop2 = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "\n",
    "#输出层softmax层\n",
    "W_fc3 = weight_variable([512, 3])\n",
    "b_fc3 = bias_variable([3])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop2, W_fc3) + b_fc3\n",
    "\n",
    "\n",
    "starter_learning_rate = 0.0001\n",
    "steps_per_decay = 10\n",
    "decay_factor = 0.9\n",
    " \n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(learning_rate = starter_learning_rate,\n",
    "                                           global_step = global_step,\n",
    "                                           decay_steps = steps_per_decay,\n",
    "                                           decay_rate = decay_factor,\n",
    "                                           staircase = True,#If `True` decay the learning rate at discrete intervals\n",
    "                                           #staircase = False,change learning rate at every step\n",
    "                                           )\n",
    "  \n",
    "# cross_entropy = tf.reduce_sum(tf.square(y_-y_conv)) \n",
    "# cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "cross_entropy = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=y_conv))\n",
    "tf.summary.scalar(\"cross_entropy\",cross_entropy)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "batch_size = 4\n",
    "n_batch = len(labels) // batch_size + 1\n",
    "                \n",
    "checkpointsPath = \"./classs_checkpoints/\"\n",
    "reload = True\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #保存训练好的模型一遍下次使用\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(\"logs_dir/\", sess.graph)\n",
    "    \n",
    "    if not os.path.exists(checkpointsPath):\n",
    "        os.mkdir(checkpointsPath)\n",
    "    \n",
    "    if reload:\n",
    "        checkPoint = tf.train.get_checkpoint_state(checkpointsPath)\n",
    "        if checkPoint and checkPoint.model_checkpoint_path:\n",
    "            saver.restore(sess,checkPoint.model_checkpoint_path)\n",
    "            print(\"restored %s\" % checkPoint.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"no checkpoint found!\")\n",
    "        \n",
    "    for i in tqdm(range(300)):\n",
    "#         print(\"opoch:\",epoch)\n",
    "#         for i in tqdm(range(10000)):\n",
    "        batch_x,batch_y = next_batch(batch_size, images, labels)\n",
    "        summary,_ = sess.run([merged,train_step],feed_dict={input_image:batch_x, y_:batch_y, keep_prob:0.8})\n",
    "        writer.add_summary(summary, i)\n",
    "        #保存训练好的模型函数　第三个参数是想将训练的次数作为后缀加入到文件的名称中去\n",
    "#         saver.save(sess, checkpointsPath +  \"/save_net.ckpt\")\n",
    "        if i % 100 == 0: # prevent save at the beginning\n",
    "            print(\"save model\")\n",
    "            saver.save(sess, os.path.join(checkpointsPath,\"saved_net.ckpt\"), global_step=i)\n",
    "            batch_x,batch_y = next_batch(batch_size, images, labels)\n",
    "            print(\"loss is:\",sess.run(cross_entropy,feed_dict={input_image:batch_x, y_:batch_y, keep_prob:1.0}))\n",
    "    print(\"done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
